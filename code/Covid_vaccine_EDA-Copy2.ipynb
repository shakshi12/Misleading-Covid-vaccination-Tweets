{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.ticker as mtick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text2emotion as te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid scientific notations\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hatesonar import Sonar\n",
    "sonar = Sonar()\n",
    "#sonar.ping(text=\"At least I'm not a nigger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeled data from XLNet\n",
    "vaccine = pd.read_csv('data_forensics_part2_after_followers_count_stemming.csv', encoding = 'latin-1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine.drop('Unnamed: 0', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vaccine.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vaccine['reply_to'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine = vaccine.mask(vaccine.applymap(str).eq('[]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "vaccine['token_clean'] =  vaccine['clean'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "%vaccine['token_clean'] = vaccine['token_clean'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import * \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "vaccine['stem_clean'] = vaccine['token_clean'].apply(lambda x: [lemmatizer.lemmatize(i) for i in x]) # stemming\n",
    "stemmer = SnowballStemmer('english')\n",
    "vaccine['stem_clean'] = vaccine['token_clean'].apply(lambda x: [stemmer.stem(i) for i in x]) # stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['stem_clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list to string\n",
    "vaccine['string_stem_clean'] = [','.join(map(str, l)) for l in vaccine['stem_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list to string\n",
    "vaccine['token_clean'] = [','.join(map(str, l)) for l in vaccine['token_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modals_list = ['can',\n",
    "'could',\n",
    "'may',\n",
    "'might',\n",
    "'must',\n",
    "'shall',\n",
    "'should',\n",
    "'will',\n",
    "'would',\n",
    "'said',\n",
    "'http',\n",
    "'https']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modals(x):\n",
    "    m = []\n",
    "    for w in str(x).split(' '):\n",
    "        if w not in modals_list:\n",
    "            m.append(w)\n",
    "    return \" \".join(m)\n",
    "\n",
    "vaccine['clean_modals'] = vaccine['string_stem_clean'].apply(modals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['string_stem_clean'] = vaccine['string_stem_clean'].apply(lambda x: ' '.join([w for w in str(x).split(',')]))\n",
    "vaccine['token_clean'] = vaccine['token_clean'].apply(lambda x: ' '.join([w for w in str(x).split(',')]))\n",
    "vaccine['clean_modals'] = vaccine['clean_modals'].apply(lambda x: ' '.join([w for w in str(x).split(',')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['string_stem_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine.to_csv('data_forensics_part2_after_followers_count_stemming.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misinfo_wordcloud = vaccine[vaccine.labels == 1]['clean_modals']\n",
    "misinfo_wordcloud.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtags distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misinfo_hashtags = pd.DataFrame(vaccine[vaccine['labels'] == 1]['hashtags'].value_counts()).reset_index()\n",
    "info_hashtags = pd.DataFrame(vaccine[vaccine['labels'] == 0]['hashtags'].value_counts()).reset_index()\n",
    "\n",
    "# convert list to string\n",
    "misinfo_hashtags['string_hashtags'] = [''.join(map(str, l)) for l in misinfo_hashtags['index']]\n",
    "info_hashtags['string_hashtags'] = [''.join(map(str, l)) for l in info_hashtags['index']]\n",
    "\n",
    "misinfo_hashtags['string_hashtags'] = misinfo_hashtags['string_hashtags'].apply(lambda x: ','.join([w for w in str(x).split(',')]))\n",
    "info_hashtags['string_hashtags'] = info_hashtags['string_hashtags'].apply(lambda x: ','.join([w for w in str(x).split(',')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output list\n",
    "output = []\n",
    "  \n",
    "# function used for removing nested \n",
    "# lists in python. \n",
    "def reemovNestings(l):\n",
    "    for i in l:\n",
    "        if type(i) == list:\n",
    "            reemovNestings(i)\n",
    "        else:\n",
    "            output.append(i)\n",
    "  \n",
    "reemovNestings(vaccine[vaccine['labels'] == 1]['hashtags'])\n",
    "#reemovNestings(vaccine[vaccine['labels'] == 1]['hashtags'])\n",
    "\n",
    "#reemovNestings(output)\n",
    "set(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misinfo_hashtags = set(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_hashtags = set(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_hashtags.intersection(misinfo_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misinfo_hashtags.difference(info_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_hashtags.difference(misinfo_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(vaccine[vaccine['labels'] == 1]['hashtags'].value_counts()).reset_index().head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine[vaccine['labels'] == 1]['hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['string_hashtags'] = [''.join(map(str, l)) for l in vaccine['hashtags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['string_hashtags'] = vaccine['hashtags'].apply(lambda x: ' '.join([w for w in str(x).split(',')]))\n",
    "#s = pd.Series(vaccine['string_hashtags'], dtype = \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['s'] = [''.join(map(str, l)) for l in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['string_hashtags'] = vaccine['hashtags'].str.strip('[]').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misinfo_hashtags = vaccine[vaccine.labels == 1]['string_hashtags']\n",
    "info_hashtags = vaccine[vaccine.labels == 0]['string_hashtags']\n",
    "misinfo_hashtags.dropna(inplace = True)\n",
    "info_hashtags.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#misinfo\n",
    "list_of_hashtags = []\n",
    "def remove_commas(x):\n",
    "    x = str(x)\n",
    "    list_of_hashtags.extend(([w for w in x.split(',')]))\n",
    "#s.apply(lambda x: ','.join([w for w in str(x).split(',')]))\n",
    "misinfo_hashtags.apply(remove_commas)\n",
    "#info_hashtags.apply(remove_commas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_hashtags1 = []\n",
    "def remove_commas(x):\n",
    "    x = str(x)\n",
    "    list_of_hashtags1.extend(([w for w in x.split(',')]))\n",
    "#s.apply(lambda x: ','.join([w for w in str(x).split(',')]))\n",
    "#misinfo_hashtags.apply(remove_commas)\n",
    "info_hashtags.apply(remove_commas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# present in misinfo (list of hashtags) but not in info\n",
    "# hashtags - # present in misinfo but not in info\n",
    "# hashtags - saynotopoisonvaccines, vaccineextortion, astrazenecapoison, pseudoscience, mychoice, factcheckvaccines, gatesfoundation, pfizer_or_nothing, antivaxxers, novaccine4me, nojohnsonnjohnsonvaccine4me, trumpcrimes, trumpvirusdeathtoll240k, trumppandemic, republicans, melindagates,billgatesisevil, untestedvaccine, iwillnotgetvaccinated, constitutionovercoronavirus, logicalfallacy, exclusive, novax4me, caution, abolishbigpharma\n",
    "set(list_of_hashtags).difference(list_of_hashtags1), antivaxxers, novaccine4me, nojohnsonnjohnsonvaccine4me, trumpcrimes, trumpvirusdeathtoll240k, trumppandemic, republicans, melindagates,billgatesisevil, untestedvaccine, iwillnotgetvaccinated, constitutionovercoronavirus, logicalfallacy, exclusive, novax4me, caution, abolishbigpharma\n",
    "set(list_of_hashtags).difference(list_of_hashtags1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashtags for info - # hashtags for info - fullyvaccinated, savetheplanet, healthnews, thisismyshot, getthevaccine, covid19updates, scienceisreal, yestocovid19vaccine, anyvaccine, inthenews, 2ndshot, covid19update, producethevaccine, vaccinatedandproud, iwilltakethevaccine, publichealth \n",
    "set(list_of_hashtags1).difference(list_of_hashtags)\n",
    "set(list_of_hashtags1).difference(list_of_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list_of_hashtags).intersection(list_of_hashtags1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list_of_hashtags1).intersection(list_of_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(vaccine[vaccine['labels'] == 0]['hashtags'].value_counts()).reset_index().head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = vaccine.dropna()\n",
    "misinfo_co_hashtags = s[s.labels == 1]\n",
    "info_co_hashtags = s[s.labels == 0]\n",
    "\n",
    "l = s[s.duplicated(subset = ['string_hashtags'])]['string_hashtags']\n",
    "misinfo_co = misinfo_co_hashtags[misinfo_co_hashtags.duplicated(subset = ['string_hashtags'])]['string_hashtags']\n",
    "info_co = info_co_hashtags[info_co_hashtags.duplicated(subset = ['string_hashtags'])]['string_hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 280 co-hashtags - 'bcpoli', 'covid19', 'cdnpoli', 'bcpoli', 'covid19', 'denmark', 'pa', 'montcopa', 'pfizer', 'vaccine'\n",
    "# 'vaccine', 'minors', 'covid19', 'vaxupil', 'publichealth'\n",
    "# 'vaccinerollout', 'astrazeneca'\n",
    "# 'vaccineswork', 'callyourpediatrician'\n",
    "# 'astrazeneca', 'covid19'\n",
    "\n",
    "info_co.to_csv('info_cohashtags.csv')\n",
    "# 86 co-hashtags - 'astrazeneca', 'covid19'\n",
    "\n",
    "misinfo_co.to_csv('misinfo_cohashtags.csv')\n",
    "# 377 co-hashtags\n",
    "l.to_csv('cohashtags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vaccine['hashtags_count'].apply(lambda x: 0 if x == np.nan else x)\n",
    "def hashtags_nan(x):\n",
    "    if len(str(x)) <= 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "vaccine['hashtags'] = vaccine['hashtags'].apply(hashtags_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtags_nan(x):\n",
    "    if len(str(x)) <= 3:\n",
    "        return 0\n",
    "    else:\n",
    "        vaccine['hashtags'].str.split().str.len()\n",
    "        return x\n",
    "\n",
    "vaccine['hashtags_count'] = vaccine['hashtags'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['hashtags_count'] = vaccine['hashtags_count'].apply(lambda x: 0 if pd.isnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['hashtags_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(vaccine[vaccine['labels'] == 0]['hashtags_count'].describe()))\n",
    "print(pd.DataFrame(vaccine[vaccine['labels'] == 1]['hashtags_count'].describe()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y = vaccine['hashtags_count'], x = vaccine.labels, palette=\"Set3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "plt.rcParams[\"font.family\"] = \"Helvetica\"\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\Hashtags_count_line.pdf') as export_pdf:\n",
    "\n",
    "    vaccine.groupby(by = [ 'hashtags_count', 'Labels']).size().groupby(level=0).apply(\n",
    "        lambda x: 100 * x / x.sum()).unstack().plot(kind='line',stacked=False, color=['IndianRed', 'SteelBlue'])\n",
    "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.xlabel('')\n",
    "    #plt.gcf().set_size_inches(5, 3)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "plt.rcParams[\"figure.figsize\"] = (16,5)\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\Hashtags_count_bar_stacked.pdf') as export_pdf:\n",
    "\n",
    "    vaccine.groupby(by = [ 'hashtags_count', 'Labels']).size().groupby(level=0).apply(\n",
    "        lambda x: 100 * x / x.sum()).unstack().plot(kind='bar',stacked=True, color=['IndianRed', 'SteelBlue'])\n",
    "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    plt.xticks(rotation = 0)\n",
    "    #plt.gcf().set_size_inches(5, 3)\n",
    "    plt.xlabel('')\n",
    "    plt.legend(loc = 0 , prop={'size': 3})\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "plt.rcParams[\"figure.figsize\"] = (12,5)\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\Hashtags_count_bar.pdf') as export_pdf:\n",
    "\n",
    "    vaccine.groupby(by = [ 'hashtags_count', 'Labels']).size().groupby(level=0).apply(\n",
    "        lambda x: 100 * x / x.sum()).unstack().plot(kind='bar',stacked=False, color=['IndianRed', 'SteelBlue'])\n",
    "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    plt.xticks(rotation = 0)\n",
    "    #plt.gcf().set_size_inches(5, 3)\n",
    "    plt.xlabel('')\n",
    "    plt.legend(loc = 0 , prop={'size': 16})\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine[vaccine['labels'] == 1]['hashtags_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine[vaccine['labels'] == 0]['hashtags_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['hashtags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['hashtags_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics - country-specific (eu, us, uk), side-effects, vaccines names, wait, die, efficacy\n",
    "num_topics = 5\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics - country-specific (eu, us, uk), side-effects, vaccines names, \n",
    "num_topics = 5\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5 , figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = vaccine.loc[vaccine['labels'] == 0]['clean'].values.tolist()\n",
    "data = info1\n",
    "data_words = list(sent_to_words(data))\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics - vaccines names, warp speed operation, efficacy, shot, vaccine trials\n",
    "num_topics = 5\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics - vaccines names, warp speed operation, efficacy, shot, vaccine trials\n",
    "num_topics = 5\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5 , figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign vaccine name to each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaccine_name(x):\n",
    "    v = []\n",
    "    if \"pfizer\" in x or \"biontech\" in x:\n",
    "        v.append(\"PFZ\")\n",
    "    if \"astrazeneca\" in x or \"oxford\" in x or \"astrazaneca\" in x or \"astrazanaca\" in x:\n",
    "        v.append(\"ASZ\")\n",
    "    if \"moderna\" in x:\n",
    "        v.append(\"MDN\")\n",
    "    if \"johnson and johnson\" in x or \"johnson & johnson\" in x or \"johnson\" in x or \"johansson\" in x or \"johanson\" in x:\n",
    "        v.append(\"J&J\")\n",
    "    if \"covaxin\" in x or \"bharat\" in x or \"covax\" in x:\n",
    "        v.append(\"CVX\")\n",
    "    if (len(v) == 0):\n",
    "        v.append(\"GEN\")\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['vaccines_name'] = vaccine['clean'].apply(vaccine_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if the list contains an empty list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine[vaccine['vaccines_name'].str.len() == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['vaccines_names_strings'] = [','.join(map(str, l)) for l in vaccine['vaccines_name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(sentence):\n",
    "\n",
    "\t# Create a SentimentIntensityAnalyzer object.\n",
    "\tsid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "\t# polarity_scores method of SentimentIntensityAnalyzer\n",
    "\t# oject gives a sentiment dictionary.\n",
    "\t# which contains pos, neg, neu, and compound scores.\n",
    "\tsentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "\t\n",
    "\t#print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "\t#print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "\t#print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "\t#print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    "\n",
    "\n",
    "\t#print(\"Sentence Overall Rated As\", end = \" \")\n",
    "\n",
    "\t# decide sentiment as positive, negative and neutral\n",
    "\tif sentiment_dict['compound'] >= 0.05 :\n",
    "\t\treturn (\"Positive\")\n",
    "\n",
    "\telif sentiment_dict['compound'] <= - 0.05 :\n",
    "\t\treturn (\"Negative\")\n",
    "\n",
    "\telse :\n",
    "\t\treturn (\"Neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['sentiments'] = vaccine['clean'].apply(sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['sentiments'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Analysis on misinformation kind of tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['Labels'] = vaccine['labels'].apply(lambda x: 'misinformation' if x == 1 else 'information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine[['sentiments', 'labels']].to_csv('sentiments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['Labels'] = vaccine['labels'].apply(lambda x: 'Misleading' if x == 1 else 'Non-Misleading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "plt.rcParams[\"figure.figsize\"] = (8,4)\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 16}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\Sentiments.pdf') as export_pdf:\n",
    "    \n",
    "\n",
    "    vaccine.groupby(by = ['Labels', 'sentiments']).size().groupby(level=0).apply(\n",
    "        lambda x: 100 * x / x.sum()).unstack().plot(kind='bar',stacked=False, color=['IndianRed', 'SteelBlue', 'LightSeaGreen'])\n",
    "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.xlabel('')\n",
    "    #plt.gcf().set_size_inches(5, 3)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine.groupby(by = ['Labels', 'sentiments']).size().groupby(level=0).apply(\n",
    "        lambda x: 100 * x / x.sum()).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "classes = vaccine['Labels'].value_counts()\n",
    "classes\n",
    "class_0 = classes[0]/vaccine['Labels'].count()*100\n",
    "class_1 = classes[1]/vaccine['Labels'].count()*100\n",
    "dist_df = pd.DataFrame({'Percentage':[class_0,class_1]},index=['Non-Misleading','Misleading'])\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\Classes_count.pdf') as export_pdf:\n",
    "#plt.title(\"Percentage of News\",fontweight='bold')\n",
    "    sns.barplot(x=dist_df.index,y=dist_df['Percentage'],palette='Paired')\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Misinformation count with vaccines name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vaccine.groupby(['misinfo_tags_strings', 'vaccines_name']).sum()\n",
    "#df.value_counts([\"Group\", \"Size\"])\n",
    "\n",
    "pd.DataFrame(vaccine.groupby([\"labels\", \"vaccines_names_strings\"]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(20, 17))\n",
    "ax = plt.axes()\n",
    "sns.countplot(hue = vaccine['labels'], y = vaccine['vaccines_names_strings'], palette = \"Set1\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccines_names = vaccine[vaccine['vaccines_names_strings'] != 'GEN']\n",
    "vaccine['vaccines_names_strings'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "def overlap_count(x):\n",
    "    if str(x) == 'GEN':\n",
    "        return 5\n",
    "    if str(x) == 'CVX':\n",
    "        return 0\n",
    "    else:\n",
    "        return str(x).count(',') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['Overlap_count'] = vaccine['vaccines_names_strings'].apply(overlap_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['Overlap_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['Overlap_count'].apply(lambda x: 5 if x == 4 else x).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vaccines_names['Overlapping_NonOverlapping'] = vaccines_names['vaccines_names_strings'].apply(lambda x: 'Overlapping' if ',' in x else 'Non-Overlapping')\n",
    "vaccines_names['Overlap_Count'] = vaccines_names['vaccines_names_strings'].str.count(',').add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vaccines_names['Overlap_Count'] = vaccine['vaccines_names_strings'].str.count(',').add(1)\n",
    "vaccines_names['Overlap_Count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccines_names['Overlap_Count'] = vaccine['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (18,12)\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\Vaccine_names.pdf') as export_pdf:\n",
    "\n",
    "    vaccines_names.groupby(by = [ 'vaccines_names_strings', 'Labels']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).unstack().plot(kind='bar',stacked=True, color=['Pink', 'DarkSeaGreen'])\n",
    "\n",
    "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    #plt.legend( ncol=3)\n",
    "    plt.xticks(rotation = 45)\n",
    "    #plt.gcf().set_size_inches(5, 3)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 32}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\Vaccine_names_overlap.pdf') as export_pdf:\n",
    "\n",
    "    vaccine.groupby(by = [ 'Overlap_count', 'Labels']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).unstack().plot(kind='bar',stacked=True, color=['Crimson', 'Green'])\n",
    "\n",
    "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    #plt.legend( ncol=3)\n",
    "    plt.xticks(rotation = 0)\n",
    "    #plt.gcf().set_size_inches(5, 3)\n",
    "    #plt.legend(loc = 0)\n",
    "    plt.xlabel('')\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "    plt.gca().get_legend().remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "font = {'family' : 'serif',\n",
    "        'size'   :22}\n",
    "plt.rc('font', **font)\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\pievaccineoverlap.pdf') as export_pdf:\n",
    "\n",
    "    # Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "    labels = 'None', 'One', 'Two', 'Three', 'Four', 'Five'\n",
    "    sizes = [1040, 6775, 4105, 6466, 7320, 5989]\n",
    "    #explode = (0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n",
    "\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "            shadow=True, startangle=0)\n",
    "    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m(y):\n",
    "    for i in range(0, len(labels)):\n",
    "        if (labels[i] == 0 ):\n",
    "            return 100* (y/75634)\n",
    "        else:\n",
    "            return 100* (y/39001)\n",
    "        \n",
    "    \n",
    "vaccines_names['overlap_countlabelwise'] = vaccines_names['Overlap_Count'].apply(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccines_names['misinfo_percent'] = (100 * vaccines_names[vaccines_names.labels == 1]['Overlap_Count'] / \n",
    "                  vaccines_names[vaccines_names.labels == 1]['Overlap_Count'].sum())\n",
    "vaccines_names['info_percent'] = (100 * vaccines_names[vaccines_names.labels == 0]['Overlap_Count'] / \n",
    "                  vaccines_names[vaccines_names.labels == 0]['Overlap_Count'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccines_names['misinfo_percent'].fillna(0, inplace = True)\n",
    "vaccines_names['info_percent'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.DataFrame(vaccines_names['info_percent'])\n",
    "b.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccines_names['misinfo_percent'] = vaccines_names['misinfo_percent'].round(decimals = 3)\n",
    "vaccines_names['info_percent'] = vaccines_names['info_percent'].round(decimals = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = vaccines_names[vaccines_names.labels == 0]['Overlap_Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = vaccines_names[vaccines_names.labels == 1]['Overlap_Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccines_names.groupby(by = [ 'misinfo_percent', 'info_percent', 'Labels']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (18,4)\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 16}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\Vaccine_names_overlap_label.pdf') as export_pdf:\n",
    "\n",
    "    vaccines_names.groupby(by = [ 'misinfo_percent', 'info_percent', 'Labels']).size().plot(kind='bar',stacked=True, color=['Pink', 'DarkSeaGreen'])\n",
    "\n",
    "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    #plt.legend( ncol=3)\n",
    "    plt.xticks(rotation = 0)\n",
    "    #plt.gcf().set_size_inches(5, 3)\n",
    "    #plt.legend(loc = 0)\n",
    "    plt.xlabel('')\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,4)\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 16}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\Vaccine_names_overlap_unstacked.pdf') as export_pdf:\n",
    "\n",
    "    vaccines_names.groupby(by = [ 'Overlap_Count', 'Labels']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).unstack().plot(kind='bar',stacked=False, color=['Pink', 'DarkSeaGreen'])\n",
    "\n",
    "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    #plt.legend( ncol=3)\n",
    "    plt.xticks(rotation = 0)\n",
    "    #plt.gcf().set_size_inches(5, 3)\n",
    "    #plt.legend(loc = 0)\n",
    "    plt.xlabel('')\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['emotions'] = vaccine['clean'].apply(lambda text: te.get_emotion(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotions(x):\n",
    "    b = 'null'\n",
    "    m = max(x['Happy'], x['Angry'], x['Surprise'], x['Sad'], x['Fear'])\n",
    "    if (m == x['Happy']):\n",
    "        a = 'Happy'  \n",
    "    elif (m == x['Angry']):\n",
    "        a = 'Angry'\n",
    "    elif (m == x['Surprise']):\n",
    "        a = 'Surprise'\n",
    "    elif (m == x['Sad']):\n",
    "        a = 'Sad'\n",
    "    elif (m == x['Fear']):\n",
    "        a = 'Fear'\n",
    "    return a\n",
    "\n",
    "vaccine['emotions_class'] = vaccine['emotions'].apply(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "vaccine.groupby(by = ['emotions_class', 'labels']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).unstack().plot(kind='bar',stacked=True)\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "vaccine.groupby(by = ['NN_count', 'labels']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).unstack().plot(kind='bar',stacked=True)\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "#plt.figure(figsize = (2,5))\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 14}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\Emotions.pdf') as export_pdf:\n",
    "    \n",
    "\n",
    "    vaccine.groupby(by = ['Labels', 'emotions_class']).size().groupby(level=0).apply(\n",
    "        lambda x: 100 * x / x.sum()).unstack().plot(kind='bar',stacked=False, color=['SlateGray', 'IndianRed', 'CadetBlue', 'PeachPuff', 'LightSalmon'])\n",
    "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.xlabel('')\n",
    "    #plt.gcf().legend()\n",
    "    plt.gcf().set_size_inches(7, 3)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    plt.legend(['Anger', 'Fear', 'Happiness', 'Sadness', 'Surprise'], bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine.groupby(by = ['Labels', 'emotions_class']).size().groupby(level=0).apply(\n",
    "        lambda x: 100 * x / x.sum()).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(vaccine.groupby(by = ['Labels', 'emotions_class']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Anger', 'Fear', 'Happiness', 'Sadness', 'Surprise']\n",
    "categories = [*categories, categories[0]]\n",
    "\n",
    "r_1 = [3.865, 32.524, 18.352, 12.862, 32.398]\n",
    "r_2 = [4.467, 39.527, 12.910, 16.710, 26.387]\n",
    "r_1 = [*r_1, r_1[0]]\n",
    "r_2 = [*r_2, r_2[0]]\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Scatterpolar(r=r_1, theta=categories, fill='toself', line={'color':'rgba(0, 128, 0, 0.4)'}, name='Non-Misleading'),\n",
    "        go.Scatterpolar(r=r_2, theta=categories, fill='toself', line={'color':'rgba(220, 20, 60, 0.4)'}, name='Misleading'),\n",
    "        #go.Scatterpolar(r=restaurant_3, theta=categories, name='Restaurant 3')\n",
    "    ],\n",
    "    layout=go.Layout(\n",
    "        #title=go.layout.Title(text='Emotions'),\n",
    "        polar={'radialaxis': {'visible': False}},\n",
    "        #angularaxis = {'ticksuffix': 25},\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "        \n",
    ")\n",
    "fig.update_layout(\n",
    "    #title=\"Plot Title\",\n",
    "    #xaxis_title=\"X Axis Title\",\n",
    "    #yaxis_title=\"Y Axis Title\",\n",
    "    #legend_title=\"Legend Title\",\n",
    "    font=dict(\n",
    "        family=\"serif\",\n",
    "        size=40,\n",
    "        #color=\"RebeccaPurple\"\n",
    "    )\n",
    "        )\n",
    "\n",
    "pyo.plot(fig)\n",
    "#fig.write_image(\"radar_emo.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "#plt.figure(figsize = (2,5))\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 14}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\Emotions_stacked.pdf') as export_pdf:\n",
    "    \n",
    "\n",
    "    vaccine.groupby(by = ['Labels', 'emotions_class']).size().groupby(level=0).apply(\n",
    "        lambda x: 100 * x / x.sum()).unstack().plot(kind='bar',stacked=True, color=['SlateGray', 'IndianRed', 'CadetBlue', 'PeachPuff', 'LightSalmon'])\n",
    "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.xlabel('')\n",
    "    #plt.gcf().legend()\n",
    "    plt.gcf().set_size_inches(7, 3)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    plt.legend(['Anger', 'Fear', 'Happiness', 'Sadness', 'Surprise'], bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('save_as_a_png.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(hue = vaccine['labels'], x = vaccine['emotions_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouns, Pronouns and Adjectives\n",
    "def pronoun_count(x):\n",
    "    tokens = word_tokenize(x)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    counts = Counter(tag for word,tag in tagged)\n",
    "    return counts['PRP']\n",
    "#total = sum(counts.values())\n",
    "#dict((word, float(count)/total) for word,count in counts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['PRP_count'] = vaccine['clean'].apply(pronoun_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouns, Pronouns and Adjectives\n",
    "def noun_count(x):\n",
    "    tokens = word_tokenize(x)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    counts = Counter(tag for word,tag in tagged)\n",
    "    return counts['NN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['NN_count'] = vaccine['clean'].apply(noun_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouns, Pronouns and Adjectives\n",
    "def adj_count(x):\n",
    "    tokens = word_tokenize(x)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    counts = Counter(tag for word,tag in tagged)\n",
    "    return counts['JJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['JJ_count'] = vaccine['clean'].apply(adj_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives_prp = vaccine[vaccine['labels'] == 1]['PRP_count']\n",
    "positives_prp = vaccine[vaccine['labels'] == 0]['PRP_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(negatives_prp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(positives_prp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot( x = vaccine['PRP_count'], kind=\"kde\", hue = vaccine['labels'], fill = True, palette = 'Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives_stop = vaccine[vaccine['labels'] == 1]['stop_words_count']\n",
    "positives_stop = vaccine[vaccine['labels'] == 0]['stop_words_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot( x = vaccine['stop_words_count'], kind=\"kde\", hue = vaccine['labels'], fill = True, palette = 'Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot( x=vaccine['stop_words_count'], hue=vaccine['labels'], kind=\"ecdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives_nn = vaccine[vaccine['labels'] == 1]['NN_count']\n",
    "positives_nn = vaccine[vaccine['labels'] == 0]['NN_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(negatives_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(positives_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot( x = vaccine['NN_count'], kind=\"kde\", hue = vaccine['labels'], fill = True, palette = 'Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot( x=vaccine['NN_count'], hue=vaccine['labels'], kind=\"ecdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives_jj = vaccine[vaccine['labels'] == 1]['JJ_count']\n",
    "positives_jj = vaccine[vaccine['labels'] == 0]['JJ_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot( x = vaccine['JJ_count'], kind=\"kde\", hue = vaccine['labels'], fill = True, palette = 'Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot( x = vaccine['JJ_count'], kind=\"kde\", fill = True, palette = 'Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot( x=vaccine['JJ_count'], hue=vaccine['labels'], kind=\"ecdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=vaccine['JJ_count'], y=vaccine['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shifterator as sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WH words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wh_count(x):\n",
    "    tokens = word_tokenize(x)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    counts = Counter(tag for word,tag in tagged)\n",
    "    return (counts['WRB'] + counts['WP'] + counts['WP$'] + counts['WDT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['WH_count'] = vaccine['clean'].apply(wh_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['WH_count'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adverb_count(x):\n",
    "    tokens = word_tokenize(x)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    counts = Counter(tag for word,tag in tagged)\n",
    "    return (counts['RB'] + counts['RBR'] + counts['RBS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['ADV_count'] = vaccine['clean'].apply(adverb_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['ADV_count'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conj_count(x):\n",
    "    tokens = word_tokenize(x)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    counts = Counter(tag for word,tag in tagged)\n",
    "    return (counts['CC'] + counts['IN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['Conj_count'] = vaccine['clean'].apply(conj_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['Conj_count'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_count(x):\n",
    "    tokens = word_tokenize(x)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    counts = Counter(tag for word,tag in tagged)\n",
    "    return (counts['VB'] + counts['VBD']+ counts['VBG']+ counts['VBN']+ counts['VBP']+ counts['VBZ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['VERB_count'] = vaccine['clean'].apply(verb_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['VERB_count'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_count(x):\n",
    "    tokens = word_tokenize(x)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    counts = Counter(tag for word,tag in tagged)\n",
    "    return (counts['DT'] + counts['PDT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['DT_count'] = vaccine['clean'].apply(dt_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['DT_count'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proper Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PROPERNOUNS_count(x):\n",
    "    tokens = word_tokenize(x)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    counts = Counter(tag for word,tag in tagged)\n",
    "    return (counts['NNP'] + counts['NNPS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['NNP_count'] = vaccine['clean'].apply(PROPERNOUNS_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['NNP_count'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type token ratio:\n",
    "For any paragraph of text, the no. of types is the number of unique tokens (words) contained in that paragraph while the no. of tokens is the total number of words in that paragraph. *\n",
    "TTR= \\left (\\frac{\\sum \\left ( No. of Types \\right )}{No.of Tokens}\\right ) * 100\n",
    "\n",
    "TTR is a measure of the lexical diversity (and some say, hence quality) of a text. Which makes total logical sense right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TTR_count(x):\n",
    "    document= re.sub(r'[^\\w]', ' ', x)\n",
    "    document=document.lower()\n",
    "    tokens=word_tokenize(document)\n",
    "    types=Counter(tokens)\n",
    "    TTR= (len(types)/len(tokens))*100\n",
    "    return TTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['TTR_count'] = vaccine['tweet'].apply(TTR_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['TTR_count'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\TTR.pdf') as export_pdf:\n",
    "    \n",
    "    sns.violinplot(vaccine.Labels, vaccine.TTR_count, palette = 'Set2')\n",
    "    plt.xticks(rotation = 0)\n",
    "    #plt.gcf().set_size_inches(5, 3)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(vaccine[vaccine.Labels == 'Misleading']['TTR_count'], alpha=0.5, label=\"Misleading\")\n",
    "plt.hist(vaccine[vaccine.Labels == 'Non-Misleading']['TTR_count'], alpha=0.5, label=\"Non-Misleading\")\n",
    "plt.xlabel(\"Type-Token Ratio\", size=14)\n",
    "plt.ylabel(\"Count\", size=14)\n",
    "#plt.title(\"Multiple Histograms with Matplotlib\")\n",
    "plt.legend(loc='upper left')\n",
    "#plt.savefig(\"overlapping_histograms_with_matplotlib_Python.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\ADV.pdf') as export_pdf:\n",
    "    \n",
    "    sns.violinplot(vaccine.Labels, vaccine['ADV_count'], palette = 'Set2')\n",
    "    plt.xticks(rotation = 0)\n",
    "    #plt.gcf().set_size_inches(5, 3)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\_ADV_normal.pdf') as export_pdf:\n",
    "    \n",
    "    sns.displot( x = undersampled['ADV_count'], kind=\"kde\", hue = undersampled['Labels'], fill = False, bw_adjust=2, common_norm = False)\n",
    "    plt.xticks(rotation = 0)\n",
    "    #plt.gcf().set_size_inches(5, 3)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\ADV_hist2.pdf') as export_pdf:\n",
    "    \n",
    "    #sns.displot(vaccine, x=\"ADV_count\", hue=\"Labels\", stat=\"density\")\n",
    "    sns.displot(vaccine, x=\"ADV_count\", hue=\"Labels\", multiple=\"dodge\", bins = 15)\n",
    "\n",
    "    \n",
    "    plt.xticks(rotation = 0)\n",
    "    #plt.gcf().set_size_inches(5, 3)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\hashtags_hist.pdf') as export_pdf:\n",
    "    \n",
    "    #sns.displot(vaccine, x=\"ADV_count\", hue=\"Labels\", stat=\"density\")\n",
    "    sns.displot(vaccine, x=\"hashtags_count\", hue=\"Labels\", multiple=\"dodge\", bins = 25, kind = \"hist\", stat = \"density\", common_norm = False)\n",
    "\n",
    "    \n",
    "    plt.xticks(rotation = 0)\n",
    "    #plt.gcf().set_size_inches(5, 3)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\ADV_hist2.pdf') as export_pdf:\n",
    "    \n",
    "    #sns.displot(vaccine, x=\"ADV_count\", hue=\"Labels\", stat=\"density\")\n",
    "    sns.displot(vaccine, x=\"ADV_count\", kind = 'hist', hue=\"Labels\"\n",
    "                , bins = 10, stat=\"density\",common_norm=False, element=\"step\")\n",
    "\n",
    "    \n",
    "    plt.xticks(rotation = 0)\n",
    "    #plt.gcf().set_size_inches(5, 3)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (18,11)\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 20}\n",
    "plt.rc('font', **font)\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\DT_normal.pdf') as export_pdf:\n",
    "    \n",
    "    g = sns.displot( x = vaccine['DT_count'], kind=\"kde\", hue = vaccine['Labels'], fill = False, bw_adjust=2.5, common_norm = False)\n",
    "    g._legend.remove()\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.gcf().set_size_inches(5, 3.5)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (18,11)\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 20}\n",
    "plt.rc('font', **font)\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\Verbs_normal.pdf') as export_pdf:\n",
    "    \n",
    "    g = sns.displot( x = vaccine['VERB_count'], kind=\"kde\", hue = vaccine['Labels'], fill = False, bw_adjust=2.5,common_norm = False)\n",
    "    g._legend.remove()\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.gcf().set_size_inches(5, 3.5)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (18,11)\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 20}\n",
    "plt.rc('font', **font)\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\CNJ_normal.pdf') as export_pdf:\n",
    "    \n",
    "    g = sns.displot( x = vaccine['Conj_count'], kind=\"kde\", hue = vaccine['Labels'], fill = False,bw_adjust=2.5, common_norm = False)\n",
    "    g._legend.remove()\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.gcf().set_size_inches(5, 3.5)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (18,11)\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 20}\n",
    "plt.rc('font', **font)\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\stopwords.pdf') as export_pdf:\n",
    "    \n",
    "    g = sns.displot( x = vaccine['stop_words_count'], kind=\"kde\", hue = vaccine['Labels'], fill = False, common_norm = False)\n",
    "    g._legend.remove()\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.gcf().set_size_inches(5, 3.5)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (18,11)\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 20}\n",
    "plt.rc('font', **font)\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\PRP_normal.pdf') as export_pdf:\n",
    "    \n",
    "    g = sns.displot( x = vaccine['PRP_count'], kind=\"kde\", hue = vaccine['Labels'], fill = False,bw_adjust=2.5, common_norm = False)\n",
    "    g._legend.remove()\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.gcf().set_size_inches(5, 3.5)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\NNP_normal.pdf') as export_pdf:\n",
    "    \n",
    "    sns.displot( x = vaccine['NNP_count'], kind=\"kde\", hue = vaccine['Labels'], fill = False, common_norm = False)\n",
    "    plt.xticks(rotation = 0)\n",
    "    #plt.gcf().set_size_inches(5, 3)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (18,11)\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 20}\n",
    "plt.rc('font', **font)\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\JJ_normal.pdf') as export_pdf:\n",
    "    \n",
    "    g = sns.displot( x = vaccine['JJ_count'], kind=\"kde\", hue = vaccine['Labels'], fill = False,bw_adjust=2.5, common_norm = False)\n",
    "    g._legend.remove()\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.gcf().set_size_inches(5, 3.5)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (18,11)\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 20}\n",
    "plt.rc('font', **font)\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\WH_normal_with_labels.pdf') as export_pdf:\n",
    "    \n",
    "    g = sns.displot( x = vaccine['WH_count'], kind=\"kde\", hue = vaccine['Labels'], fill = False, common_norm = False)\n",
    "    #g._legend.remove()\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.gcf().set_size_inches(5, 3.5)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.gcf().legend(bbox_to_anchor=(0.6,1.4), loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\WH.pdf') as export_pdf:\n",
    "    \n",
    "    sns.violinplot(vaccine.Labels, vaccine['WH_count'], palette = 'Set2')\n",
    "    plt.xticks(rotation = 0)\n",
    "    #plt.gcf().set_size_inches(5, 3)\n",
    "    #plt.legend(loc = 0)\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (18,11)\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 20}\n",
    "plt.rc('font', **font)\n",
    "with PdfPages(r'C:\\Users\\shakshi\\Documents\\RESEARCH\\General_code\\antivaccine_mostly\\pdfs\\TTR_count_normal.pdf') as export_pdf:\n",
    "    \n",
    "    g = sns.displot( x = vaccine['TTR_count'], kind=\"kde\", hue = vaccine['Labels'], fill = False, common_norm = False)\n",
    "    g._legend.remove()\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.gcf().set_size_inches(5, 3.5)\n",
    "    #plt.legend(loc = 3)\n",
    "   # plt.legend('')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    #plt.legend(bbox_to_anchor=(1,1.2), loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.gcf().subplots_adjust(bottom=0.04)\n",
    "    export_pdf.savefig(bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shift Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine[vaccine['labels'] == 1]['clean_modals_stopwords2'].to_csv('clean_text_for_shift_iterator_misinfo.txt', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine[vaccine['labels'] == 0]['clean_modals_stopwords2'].to_csv('clean_text_for_shift_iterator_info.txt', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in read mode\n",
    "text = open(\"clean_text_for_shift_iterator_info.txt\", \"r\")\n",
    "  \n",
    "# Create an empty dictionary\n",
    "type2freq_1 = dict()\n",
    "  \n",
    "# Loop through each line of the file\n",
    "for line in text:\n",
    "    # Remove the leading spaces and newline character\n",
    "    line = line.strip()\n",
    "  \n",
    "    # Convert the characters in line to \n",
    "    # lowercase to avoid case mismatch\n",
    "    line = line.lower()\n",
    "  \n",
    "    # Split the line into words\n",
    "    words = line.split(\" \")\n",
    "    filtered_sentence = []\n",
    "    for w in words: \n",
    "        if w not in stopwords3: \n",
    "            filtered_sentence.append(w) \n",
    "  \n",
    "    # Iterate over each word in line\n",
    "    for word in filtered_sentence:\n",
    "        # Check if the word is already in dictionary\n",
    "        if word in type2freq_1:\n",
    "            # Increment count of word by 1\n",
    "            type2freq_1[word] = type2freq_1[word] + 1\n",
    "        else:\n",
    "            # Add the word to dictionary with count 1\n",
    "            type2freq_1[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vaccine.clean[40].split(\" \")\n",
    "filtered_sentence = []\n",
    "for w in words: \n",
    "    if w not in stopwords3: \n",
    "        filtered_sentence.append(w) \n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in read mode\n",
    "text = open(\"clean_text_for_shift_iterator_misinfo.txt\", \"r\")\n",
    "  \n",
    "# Create an empty dictionary\n",
    "type2freq_2 = dict()\n",
    "  \n",
    "# Loop through each line of the file\n",
    "for line in text:\n",
    "    # Remove the leading spaces and newline character\n",
    "    line = line.strip()\n",
    "  \n",
    "    # Convert the characters in line to \n",
    "    # lowercase to avoid case mismatch\n",
    "    line = line.lower()\n",
    "  \n",
    "    # Split the line into words\n",
    "    words = line.split(\" \")\n",
    "    filtered_sentence = []\n",
    "    for w in words: \n",
    "        if w not in stopwords3: \n",
    "            filtered_sentence.append(w) \n",
    "  \n",
    "  \n",
    "    # Iterate over each word in line\n",
    "    for word in words:\n",
    "        # Check if the word is already in dictionary\n",
    "        if word in type2freq_2:\n",
    "            # Increment count of word by 1\n",
    "            type2freq_2[word] = type2freq_2[word] + 1\n",
    "        else:\n",
    "            # Add the word to dictionary with count 1\n",
    "            type2freq_2[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_shift = sh.WeightedAvgShift(type2freq_1,\n",
    "                                      type2freq_2,\n",
    "                                      'labMT_English',\n",
    "                                     stop_lens=[(4,6)])\n",
    "sentiment_shift.get_shift_graph(system_names = ['Non-Misleading', 'Misleading'],\n",
    "                                  top_n =  50, height = 9,show_total = True,\n",
    "                                 show_plot = True,text_size_inset = True, detailed = True, filename = 'sentiments_MT.pdf',\n",
    "                               cumulative_inset = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_shift = sh.WeightedAvgShift(type2freq_1,\n",
    "                                      type2freq_2,\n",
    "                                      'NRC-emotion_fear_English')\n",
    "sentiment_shift.get_shift_graph(system_names = ['Non-Misleading', 'Misleading'],\n",
    "                                  top_n =  50, height = 9,show_total = False,\n",
    "                                 show_plot = False,text_size_inset = False, detailed = False, filename = 'sentiments_NCR.pdf',\n",
    "                               cumulative_inset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_shift = sh.WeightedAvgShift(type2freq_1,\n",
    "                                      type2freq_2,\n",
    "                                      'NRC-emotion_surprise_English')\n",
    "sentiment_shift.get_shift_graph(system_names = ['Non-Misleading', 'Misleading'],\n",
    "                                 top_n =  50, height = 9,show_total = False,\n",
    "                                 show_plot = False,text_size_inset = False, detailed = False, filename = 'emotions_NCR_surprise.pdf',\n",
    "                               cumulative_inset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_shift = sh.WeightedAvgShift(type2freq_1,\n",
    "                                      type2freq_2,\n",
    "                                      'NRC-emotion_trust_English')\n",
    "sentiment_shift.get_shift_graph(system_names = ['Non-Misleading', 'Misleading'],\n",
    "                                 top_n =  50, height = 9,show_total = False,\n",
    "                                 show_plot = False,text_size_inset = False, detailed = False, filename = 'emotions_NCR_trust.pdf',\n",
    "                               cumulative_inset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_shift = sh.WeightedAvgShift(type2freq_1,\n",
    "                                      type2freq_2,\n",
    "                                      'NRC-emotion_anger_English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_shift.get_shift_graph(detailed=False,\n",
    "                                system_names=['Non-Misleading', 'Misleading'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_shift = sh.WeightedAvgShift(type2freq_1,\n",
    "                                      type2freq_2,\n",
    "                                      'NRC-VAD_valence_English')\n",
    "sentiment_shift.get_shift_graph(system_names = ['Non-Misleading', 'Misleading'],\n",
    "                                 top_n =  50, height = 9,show_total = False,\n",
    "                                 show_plot = False,text_size_inset = False, detailed = False, filename = 'NRC-VAD_valence.pdf',\n",
    "                               cumulative_inset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_shift = sh.WeightedAvgShift(type2freq_1,\n",
    "                                      type2freq_2,\n",
    "                                      'NRC-VAD_Arousal_English')\n",
    "sentiment_shift.get_shift_graph(system_names = ['Non-Misleading', 'Misleading'],\n",
    "                                 top_n =  50, height = 9,show_total = False,\n",
    "                                 show_plot = False,text_size_inset = False, detailed = False, filename = 'NRC-VAD_arousal.pdf',\n",
    "                               cumulative_inset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_shift = sh.WeightedAvgShift(type2freq_1,\n",
    "                                      type2freq_2,\n",
    "                                      'NRC-VAD_Dominance_English')\n",
    "sentiment_shift.get_shift_graph(system_names = ['', ''], title_fontsize = -12, xlabel = '', ylabel = '', show_score_diffs = 0,\n",
    "                                 top_n =  30, height = 9,show_total = False, label_fontsize = 12,bar_width = 0.8, score_colors = { \"all_pos_neg\": \"#000000\",\n",
    "            \"all_pos_pos\": \"\", \"neg_s\": \"\",\n",
    "            \"neg_s_neg_p\": \"\",\n",
    "            \"neg_s_pos_p\": \"#FF0000\",\n",
    "            \"neg_total\": \"#2E8B57\",\n",
    "            \"pos_s\": \"\",\n",
    "            \"pos_s_neg_p\": \"#000000\",\n",
    "            \"pos_s_pos_p\": \"#000000\",\n",
    "            \"pos_total\": \"#DC143C\",\n",
    "            \"total\": \"#000000\",},\n",
    "                                remove_xticks = True,\n",
    "                                remove_yticks = True,\n",
    "                                \n",
    "                                serif = True,\n",
    "                                 show_plot = False,text_size_inset = False, detailed = False, filename = 'NRC-VAD_dominance.pdf',\n",
    "                               cumulative_inset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_shift = sh.ProportionShift(type2freq_1=type2freq_1,\n",
    "                                      type2freq_2=type2freq_2)\n",
    "proportion_shift.get_shift_graph(system_names = ['Non-Misleading', 'Misleading'],\n",
    "                                 top_n =  30, height = 9,\n",
    "                                 show_plot = False,text_size_inset = False, detailed = False, filename = 'NRC_proprtionshift.pdf',\n",
    "                               cumulative_inset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "a_file = open(\"misinfo_freq_word.pkl\", \"wb\")\n",
    "#pickle.dump(type2freq_2, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"misinfo_freq_word.pkl\", \"rb\")\n",
    "type2freq_2 = pickle.load(a_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "a_file = open(\"info_freq_word.pkl\", \"wb\")\n",
    "#pickle.dump(type2freq_1, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"info_freq_word.pkl\", \"rb\")\n",
    "type2freq_1 = pickle.load(a_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in type2freq_1.items():\n",
    "    type2freq_1[key] = value/total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in type2freq_2.items():\n",
    "    type2freq_2[key] = value/total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in type2freq_1.items():\n",
    "    type2freq_1[key] = (value/len(type2freq_1))*len(type2freq_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type2freq_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_shift = sh.EntropyShift(type2freq_1=type2freq_1,\n",
    "                                type2freq_2=type2freq_2,\n",
    "                                base=2)\n",
    "entropy_shift.get_shift_graph(system_names = ['Non-Misleading', 'Misleading'],\n",
    "                                 top_n =  30, height = 9,\n",
    "                                 show_plot = False,text_size_inset = False, detailed = False, filename = 'NRC_entropyshift.pdf',\n",
    "                               cumulative_inset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "entropy_shift = sh.EntropyShift(type2freq_1=type2freq_1,\n",
    "                                type2freq_2=type2freq_2,\n",
    "                                base=2)\n",
    "entropy_shift.get_shift_graph(system_names = ['Non-Misleading', 'Misleading'],\n",
    "                                 top_n =  30, height = 9,\n",
    "                                 show_plot = False,text_size_inset = False, detailed = False, filename = 'NRC_entropyshift_normalized.pdf',\n",
    "                               cumulative_inset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "entropy_shift = sh.EntropyShift(type2freq_1=type2freq_1,\n",
    "                                type2freq_2=type2freq_2,\n",
    "                                base=2)\n",
    "entropy_shift.get_shift_graph(system_names = ['Non-Misleading', 'Misleading'],\n",
    "                                 top_n =  30, height = 9,\n",
    "                                 show_plot = False,text_size_inset = False, detailed = False, filename = 'NRC_entropyshift_normalized.pdf',\n",
    "                               cumulative_inset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_shift.type2score_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_shift.type2shift_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_shift = sh.EntropyShift(type2freq_1=type2freq_1,\n",
    "                                type2freq_2=type2freq_2,\n",
    "                                base=2,\n",
    "                                alpha=0.5,\n",
    "                               )\n",
    "entropy_shift.get_shift_graph(system_names = ['Non-Misleading', 'Misleading'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsd_shift = sh.JSDivergenceShift(type2freq_1=type2freq_1,\n",
    "                                 type2freq_2=type2freq_2,\n",
    "                                 weight_1=0.5,\n",
    "                                 weight_2=0.5,\n",
    "                                 base=2,\n",
    "                                 alpha=1)\n",
    "jsd_shift.get_shift_graph(system_names = ['Non-Misleading', 'Misleading'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs1 = vaccine[vaccine.labels == 0]['NN_count']\n",
    "rvs2 = vaccine[vaccine.labels == 1]['NN_count']\n",
    "stats.ks_2samp(rvs1, rvs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs1 = vaccine[vaccine.labels == 0]['JJ_count']\n",
    "rvs2 = vaccine[vaccine.labels == 1]['JJ_count']\n",
    "stats.ks_2samp(rvs1, rvs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs1 = vaccine[vaccine.labels == 0]['PRP_count']\n",
    "rvs2 = vaccine[vaccine.labels == 1]['PRP_count']\n",
    "stats.ks_2samp(rvs1, rvs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs1 = vaccine[vaccine.labels == 0]['ADV_count']\n",
    "rvs2 = vaccine[vaccine.labels == 1]['ADV_count']\n",
    "stats.ks_2samp(rvs1, rvs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs1 = vaccine[vaccine.labels == 0]['TTR_count']\n",
    "rvs2 = vaccine[vaccine.labels == 1]['TTR_count']\n",
    "stats.ks_2samp(rvs1, rvs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs1 = vaccine[vaccine.labels == 0]['stop_words_count']\n",
    "rvs2 = vaccine[vaccine.labels == 1]['stop_words_count']\n",
    "stats.ks_2samp(rvs1, rvs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs1 = vaccine[vaccine.labels == 0]['DT_count']\n",
    "rvs2 = vaccine[vaccine.labels == 1]['DT_count']\n",
    "stats.ks_2samp(rvs1, rvs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs1 = vaccine[vaccine.labels == 0]['Conj_count']\n",
    "rvs2 = vaccine[vaccine.labels == 1]['Conj_count']\n",
    "stats.ks_2samp(rvs1, rvs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['clean_modals_stopwords2'] = vaccine.string_stem_clean.apply(lambda x: ' '.join([word for word in x.split() if x not in stopwords2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def extended_tau(list_a, list_b):\n",
    "    \"\"\" Calculate the extended Kendall tau from two lists. \"\"\"\n",
    "    ranks = join_ranks(create_rank(list_a), create_rank(list_b)).fillna(len(list_a))\n",
    "    print(ranks[:20])\n",
    "    dummy_df = pd.DataFrame([{'rank_a': len(list_a), 'rank_b': len(list_b)} for i in range(len(list_a)*2-len(ranks))])\n",
    "    total_df = ranks.append(dummy_df)\n",
    "    print(total_df)\n",
    "    return scale_tau(len(list_a), stats.kendalltau(total_df['rank_a'], total_df['rank_b'])[0])\n",
    "\n",
    "def scale_tau(length, value):\n",
    "    \"\"\" Scale an extended tau correlation such that it falls in [-1, +1]. \"\"\"\n",
    "    n_0 = 2*length*(2*length-1)\n",
    "    n_a = length*(length-1)\n",
    "    n_d = n_0 - n_a\n",
    "    min_tau = (2.*n_a - n_0) / (n_d)\n",
    "    return 2*(value-min_tau)/(1-min_tau) - 1\n",
    "\n",
    "def create_rank(a):\n",
    "    \"\"\" Convert an ordered list to a DataFrame with ranks. \"\"\"\n",
    "    return pd.DataFrame(\n",
    "                  zip(a, range(len(a))),\n",
    "                  columns=['key', 'rank'])\\\n",
    "             .set_index('key')\n",
    "\n",
    "def join_ranks(rank_a, rank_b):\n",
    "    \"\"\" Join two rank DataFrames. \"\"\"\n",
    "    return rank_a.join(rank_b, lsuffix='_a', rsuffix='_b', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_tau(m, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
