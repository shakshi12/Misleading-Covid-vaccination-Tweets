{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nn = pd.read_csv(r'data_senti_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[['stop_words_count', 'PRP_count', 'NN_count', 'JJ_count',\n",
    "       'WH_count', 'ADV_count', 'avg_length',\n",
    "       'Conj_count', 'VERB_count', 'DT_count', 'TTR_count', 'hashtags_count',\n",
    "       'labels', 'sentiments_Negative', 'sentiments_Positive',\n",
    "       'emotions_class_Angry', 'emotions_class_Fear', 'emotions_class_Happy',\n",
    "       'emotions_class_Surprise']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data_senti_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "font = {'family' : 'serif',\n",
    "       'size'   : 14}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "sns.heatmap(abs(df1.corr()), cmap=\"Blues\", annot = sign,  annot_kws={'size':8})\n",
    "plt.tight_layout()\n",
    "plt.savefig('features_corr_absolute_values_senti_emo_SHAPrankingsorted_annot_big.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df1.loc[:, df1.columns != 'labels']\n",
    "y1 = df1.iloc[:, df1.columns == 'labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size = 0.10, random_state = 123, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X2.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "X_train1 = scaler.fit_transform(X_train1)\n",
    "X_test1 = scaler.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None, predictions = True)\n",
    "models, predictions = clf.fit(X_train1, X_test1, y_train1, y_test1)\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(['Stop_words', 'Prounouns', 'Nouns', 'Adjectives', 'WH_words',\n",
    "       'Adverbs', 'Avg_length', 'Conjunction', 'Verbs', 'Determiner',\n",
    "       'Type_token_ratio', 'Hashtags', 'Sentiments', 'Emotions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = KFold(5,shuffle = True, random_state=0)\n",
    "classifier = RandomForestClassifier(criterion = 'entropy', random_state = 42)\n",
    "#n_classes = df1.labels.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = KFold(5,shuffle = True, random_state=0)\n",
    "classifier1 = RandomForestClassifier(criterion = 'entropy', random_state = 42)\n",
    "#n_classes = df1.labels.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, (train, test) in enumerate(folds.split(X_train1, y_train1)):\n",
    " #   \n",
    "  #  classifier.fit(X_train1[train], y_train1[train])\n",
    "\n",
    "    y_pred = classifier.predict(X_train1[test])\n",
    "    \n",
    "    conf_mat = confusion_matrix(y_train1[test], y_pred)\n",
    "    print (conf_mat)\n",
    "\n",
    "    precision = precision_score(y_train1[test], y_pred, average='micro')\n",
    "    print('Precision micro: %.3f' % precision)\n",
    "    precision = precision_score(y_train1[test], y_pred, average='macro')\n",
    "    print('Precision macro: %.3f' % precision)\n",
    "    recall = recall_score(y_train1[test], y_pred, average='micro')\n",
    "    print('Recall micro: %.3f' % recall)\n",
    "    recall = recall_score(y_train1[test], y_pred, average='macro')\n",
    "    print('Recall macro: %.3f' % recall)\n",
    "    print(accuracy_score(y_train1[test], y_pred))\n",
    "    print(classification_report(y_train1[test], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cv = classifier.predict(X_test2)\n",
    "print(classification_report(y_test2, y_pred_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = classification_report(y_test2, y_pred_cv, output_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap_values_test_senti_emo = shap.TreeExplainer(classifier).shap_values(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_test = shap.TreeExplainer(classifier).shap_values(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_colsnames = pd.DataFrame(X_test2, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.summary_plot(shap_values_test_senti_emo.iloc[:10], X_test2.iloc[:10])\n",
    "shap.summary_plot(shap_values_test_senti_emo[:10], features_colsnames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_test2 = shap.TreeExplainer(classifier1).shap_values(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap_values = shap.TreeExplainer(classifier).shap_values(X_train1)\n",
    "shap.summary_plot(shap_values_test.iloc[:10], X_test2.iloc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_colsnames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_test[:10], features_colsnames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = shap.summary_plot(shap_values = shap_values_test[0],\n",
    "                  features = features_colsnames, show=False\n",
    "                  )\n",
    "f = plt.gcf()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('shap_info.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# senti emo\n",
    "# Class 1: Misinformation\n",
    "#plt.figure(figsize = (9, 8))\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 18}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "fig = shap.summary_plot(shap_values = shap_values_test_senti_emo[1],\n",
    "                  features = features_colsnames, show=False\n",
    "                  )\n",
    "f = plt.gcf()\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_misinfo_senti_emo.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_test_senti_emo.to_csv('shap_values_test_senti_emo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class 1: Misinformation\n",
    "fig = shap.summary_plot(shap_values = shap_values_test[1],\n",
    "                  features = features_colsnames, show=False\n",
    "                  )\n",
    "f = plt.gcf()\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_misinfo.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.emotions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions and put them with the test data.\n",
    "X_output = pd.DataFrame(X_test2).copy()\n",
    "X_output.loc[:,'predict'] = np.round(classifier.predict(X_output),2)\n",
    "\n",
    "# Randomly pick some observations\n",
    "random_picks = np.arange(1,330,50) # Every 50 rows\n",
    "S = X_output.iloc[random_picks]\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your Jupyter notebook with initjs(), otherwise you will get an error message.\n",
    "shap.initjs()\n",
    "\n",
    "# Write in a function\n",
    "def shap_plot(j):\n",
    "    explainerModel = shap.TreeExplainer(classifier)\n",
    "    shap_values_Model = explainerModel.shap_values(S)\n",
    "    p = shap.force_plot(explainerModel.expected_value, shap_values_Model[j], S.iloc[[j]])\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.force_plot(explainerModel.expected_value[0], shap_values_test[0], show=False).savefig('shap.png')\n",
    "shap.force_plot(explainerModel.expected_value[0], shap_values_test[0], X_test2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test1, classifier.predict_proba(X_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precision_score(y_test1, y_pred_cv, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test1, y_pred_cv, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test1, y_pred_cv, average='micro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
